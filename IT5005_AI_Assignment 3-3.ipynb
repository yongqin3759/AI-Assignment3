{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 - Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridWorlds\n",
    "\n",
    "This assignment involves finding optimal policies for two grid worlds (CliffWalking and WindyGridWorld) using SARSA and Q learning. Details about WindyGridWorld (Example 6.5) and CliffWalking (Example 6.6) can be found in the following link.\n",
    "    \n",
    "    http://incompleteideas.net/book/RLbook2020.pdf\n",
    "\n",
    "\n",
    "You need gym (version 0.18) and numpy (version 1.20.1) for this assignment. The environment for both problems are provided. \n",
    "\n",
    "For Windy Grid World environemnt you also need the file 'WindyGridWorld.py'. \n",
    "\n",
    "### Task 1: Learning [5 Marks]\n",
    "\n",
    "You only need to write the codes for SARSA and Q-learning algorithms. Then do the learning in both 'CliffWalking' and 'Windy Grid World' environments. \n",
    "\n",
    "### Task 2: Analysis [5 Marks]   \n",
    "\n",
    "1. Calculate the average return across the episodes. It gives you a measure of the performance of the algorithm while learning.  \n",
    "\n",
    "2. Calculate the return after convergence. It gives you a measure of the performance after the learning is completed. \n",
    "\n",
    "3. What do you observe from these results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the necessary packages"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym==0.18\n",
    "!pip install numpy==1.20.1\n",
=======
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gym in c:\\users\\yong qin\\appdata\\roaming\\python\\python39\\site-packages (0.26.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gym) (2.0.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\yong qin\\appdata\\roaming\\python\\python39\\site-packages (from gym) (1.20.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gym) (4.11.3)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\yong qin\\appdata\\roaming\\python\\python39\\site-packages (from gym) (0.0.8)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.8.0->gym) (3.7.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in c:\\users\\yong qin\\appdata\\roaming\\python\\python39\\site-packages (1.20.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (4.64.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym\n",
    "!pip install numpy\n",
>>>>>>> Stashed changes
    "!pip install tqdm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Learning\n",
    "## Task 1a: Learning in CliffWalking Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment for CliffWalking\n",
    "\n",
    "The board is a 4x12 matrix, with (using NumPy matrix indexing):\n",
    "    [3, 0] as the start at bottom-left\n",
    "    [3, 11] as the goal at bottom-right\n",
    "    [3, 1..10] as the cliff at bottom-center\n",
    "\n",
    "Each time step incurs -1 reward, and stepping into the cliff incurs -100 reward\n",
    "and a reset to the start. If an action would take you off the grid, you remain in the previous state.\n",
    "An episode terminates when the agent reaches the goal.\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gym\n",
=======
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gym'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm \n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gym'"
     ]
    }
   ],
   "source": [
    "import gym as gym\n",
>>>>>>> Stashed changes
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "\n",
    "env = gym.make('CliffWalking-v0') # Create the environment #render_mode=\"human\"  human, ansi, \n",
    "env.reset() # reset environment to a new, random state\n",
    "env.render() # Renders the environment for visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here _x_ is the location of the agent, *o* are possible places to go to, *C* is the cliff, and *T* is the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = env.action_space.n \n",
    "num_states = env.observation_space.n \n",
    "\n",
    "print(\"Number of actions: \", num_actions)\n",
    "print(\"Number of states: \", num_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = 0 # Move up\n",
    "a = env.step(action) # This is the function we use to interact with the environment\n",
    "env.render() # Renders the environment for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 -> UP, 1 -> RIGHT, 2 -> DOWN, 3 -> LEFT\n",
    "env.reset()\n",
    "import time\n",
    "for action in [0, 1, 2, 3]:\n",
    "    print(\"Action: \", action)\n",
    "    time.sleep(1)\n",
    "    next_state, reward, is_done, info = env.step(action)     # next_state, reward, is_done, info\n",
    "    print(\"Next state: \", next_state)\n",
    "    print(\"Reward: \", reward)\n",
    "    print(\"Done: \",is_done)\n",
    "    env.render()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, each non-terminal action has a reward of -1. 0 -> UP, 1 -> RIGHT, 2 -> DOWN, 3 -> LEFT. The moment the agent falls off the cliff the reward becomes -100 and the agent resets to the start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize values \n",
    "num_episodes = 500\n",
    "lr = \n",
    "epsilon = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Q function - a simplified version is used here \n",
    "# in reality the number of states may be unknown and all states may not be reachable \n",
    "\n",
    "# hint: use num_states as the key to a dictionary of lists\n",
    "Q = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def behavioral_policy(state, Q, num_actions, epsilon):\n",
    "    # Implement the epsilon-greedy policy\n",
    "    # Don't forget the epsilon-greedy idea\n",
    "    probs =  \n",
    "    best_action = \n",
    "    probs[best_action] += \n",
    "    \n",
    "    action = np.argmax(np.random.multinomial(1, probs, size=1)[0])\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use this to check if your algorithm is correct\n",
    "for i in range(10):\n",
    "    print(behavioral_policy(0, Q, num_actions, 0.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARSA Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(env, Q, num_actions, num_episodes, epsilon, lr):\n",
    "    # Given to students\n",
    "    episode_length = [0] * num_episodes\n",
    "    total_reward_episode = [0] * num_episodes\n",
    "    \n",
    "\n",
    "    for episode in tqdm(range(num_episodes)):\n",
    "        state = env.reset()\n",
    "        is_done = False\n",
    "        # Implement SARSA\n",
    "        \n",
    "        while not is_done:\n",
    "            \n",
    "            episode_length[episode] += 1\n",
    "            total_reward_episode[episode] += reward\n",
    "    policy = {}\n",
    "    # Write code here as well\n",
    "    # Hint: use np.argmax\n",
    "\n",
    "    return Q, policy, {\"rewards\": total_reward_episode, \"length\": episode_length}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run SARSA\n",
    "optimal_sarsa_Q, sarsa_optimal_policy, sarsa_info = sarsa(env, Q, num_actions, num_episodes, epsilon, lr)\n",
    "print(\"\\nGridWorld SARSA Optimal policy: \\n\", sarsa_optimal_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, Q, num_actions, num_episodes, epsilon, lr):\n",
    "    # Given to students\n",
    "    episode_length = [0] * num_episodes\n",
    "    total_reward_episode = [0] * num_episodes\n",
    "\n",
    "   \n",
    "    for episode in tqdm(range(num_episodes)):\n",
    "        state = env.reset()\n",
    "        is_done = False\n",
    "        # Implemnt Q-Learning\n",
    "        \n",
    "        while not is_done:\n",
    "           \n",
    "\n",
    "            total_reward_episode[episode] += reward\n",
    "            episode_length[episode] += 1\n",
    "            \n",
    "    policy = {}\n",
    "    # Write the code here\n",
    "\n",
    "    return Q, policy, {\"rewards\": total_reward_episode, \"length\": episode_length}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Q-Learning \n",
    "\n",
    "optimal_Q, q_optimal_policy, q_info = q_learning(env, Q, num_actions, num_episodes, epsilon, lr)\n",
    "print(\"\\nGridWorld Q-Learning Optimal policy: \\n\", q_optimal_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell if you do not have the matplotlib library\n",
    "# !pip install matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rate(episode_length, total_reward_episode, title):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    ax[0].plot(episode_length)\n",
    "    ax[0].set_title(\"Episode Length over time\")\n",
    "    ax[0].set(xlabel=\"Episode\", ylabel=\"Length\")\n",
    "    ax[1].plot(total_reward_episode)\n",
    "    ax[1].set_title(\"Episode reward over time\")\n",
    "    ax[1].set(xlabel=\"Episode reward over time\", ylabel=\"Reward\")\n",
    "    fig.suptitle(title)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rate(sarsa_info[\"length\"], sarsa_info[\"rewards\"], \"GridWorld: SARSA\")\n",
    "plot_rate(q_info[\"length\"], q_info[\"rewards\"], \"GridWorld: Q-Learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1b: Learning in Windy Grid world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WindyGridWorld is similar to GridWorld, but with a few differences. You only need to move to the target state. But this time there is a cross-wind across the center of the grid that will push you upwards. In columns 3, 4, 5, and 8 there are winds of strength 1 while in column 6 and 7 there are winds of strength 2. For more details refer Example 6.5 in\n",
    "\n",
    " http://incompleteideas.net/book/RLbook2020.pdf\n",
    "\n",
    " You only need to change the environment and reuse the SARSA and Q-learning algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Windy Grid World environment\n",
    "from WindyGridWorld import WindyGridWorld\n",
    "env = WindyGridWorld()\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = env.action_space.n \n",
    "num_states = env.observation_space.n \n",
    "\n",
    "print(\"Number of actions: \", num_actions)\n",
    "print(\"Number of states: \", num_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play around with different learning rates epsilons, and Q initializations to see what is best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 1000\n",
    "lr = \n",
    "epsilon = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Q function - a simplified version is used here \n",
    "# in reality the number of states may be unknown and all states may not be reachable \n",
    "\n",
    "# hint: use num_states as the key to a dictionary of lists\n",
    "Q = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_sarsa_Q, sarsa_optimal_policy, sarsa_info = sarsa(env, Q, num_actions, num_episodes, epsilon, lr)\n",
    "print(\"\\n WindyGridWorld SARSA Optimal policy: \\n\", sarsa_optimal_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_Q, q_optimal_policy, q_info = q_learning(env, Q, num_actions, num_episodes, epsilon, lr)\n",
    "print(\"\\n WindyGridWorld Q-Learning Optimal policy: \\n\", q_optimal_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rate(sarsa_info[\"length\"], sarsa_info[\"rewards\"], \"GridWorld: SARSA\")\n",
    "plot_rate(q_info[\"length\"], q_info[\"rewards\"], \"GridWorld: Q-Learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Analysis (Comparison of Q-learning and SARSA learning algorithms)\n",
    "\n",
    "1. Comment on the number of episodes required to converge to the optimal policy for both environments. \n",
    "       \n",
    "2. Discuss the differences in the reward graphs.  \n",
    "\n",
    "3. Calculate the average return across the episodes for each environment. It gives a measure of the performance of the algorithm while learning (i.e., online performance).  \n",
    "\n",
    "4. Calculate the return after convergence. It gives you a measure of the performance after the learning is completed (i.e., offline performance). \n",
    "\n",
    "5. Briefly summarize your results.\n",
    " \n",
    " It is advisable to rerun the algorithm a few times to get a clearer understanding of the algorithms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
<<<<<<< Updated upstream
   "display_name": "Python 3.7.8 64-bit",
=======
   "display_name": "Python 3 (ipykernel)",
>>>>>>> Stashed changes
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< Updated upstream
   "version": "3.7.8"
  },
  "orig_nbformat": 4,
=======
   "version": "3.9.12"
  },
>>>>>>> Stashed changes
  "vscode": {
   "interpreter": {
    "hash": "9903bfc570e74e13454075f1f113ae8a5bc0cd8b51e84ac8c67812478a777a9f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
